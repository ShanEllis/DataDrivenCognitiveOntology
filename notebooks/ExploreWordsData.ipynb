{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add erpsc to path\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_path = '/Users/vassiki/Desktop/nhw17/DataDrivenCognitiveOntology/'\n",
    "#sys.path.append(os.getcwd)\n",
    "sys.path.append(my_path)\n",
    "# ^ Update the above with a link to the folder that has 'erpsc' in it, if not currently in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import erpsc\n",
    "from lisc.base import Base\n",
    "from lisc.core.db import SCDB\n",
    "from lisc.data import Data\n",
    "from lisc.data_all import DataAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add data to path\n",
    "db = SCDB(False)\n",
    "#fake_path = '/Users/vassiki/Desktop/nhw17/ERPSC_NLP/notebooks'\n",
    "db.project_path = os.path.join(os.getcwd(), 'dat')\n",
    "#db.project_path = fake_path\n",
    "db.gen_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load raw data from a particular ERP\n",
    "dat = Data('p300')\n",
    "dat.load(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus(ds):\n",
    "    \"\"\"\n",
    "    To flatten all abstract words into a long list \n",
    "    \n",
    "    TO DO: Generalize to making a corpus of authors, or any metric the author wants\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ds : Data structure returned by ERFSC when a query is submitted\n",
    "    \"\"\"\n",
    "    aPreCorpus = []\n",
    "    \n",
    "    numArticles = ds.n_articles\n",
    "    print('Iterating over %d articles for extracting words') %(ds.n_articles)\n",
    "    for AIdx in xrange(numArticles):\n",
    "        if ds.words[AIdx] != None:\n",
    "            aPreCorpus.append(ds.words[AIdx])\n",
    "        \n",
    "    \n",
    "    PreText = list(itertools.chain.from_iterable(aPreCorpus))\n",
    "    \n",
    "    Corpus = nltk.Text(PreText)\n",
    "    \n",
    "    return Corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_common_words(Corpus,NumWords):\n",
    "    \"\"\"\n",
    "    Return the most common words in the concatenated article text corpus\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Corpus: generated from keyword arguments from cognitive atlas\n",
    "    NumWords: Top N words to return\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(NumWords <= len(Corpus))\n",
    "    \n",
    "    fdist = nltk.FreqDist(Corpus)\n",
    "    common_words = fdist.most_common(NumWords)\n",
    "    header = [('Term', 'Frequency'),('-'*len('Term'),'-'*len('Frequency'))]\n",
    "    wordList = header + common_words\n",
    "    \n",
    "    width = max(len(e) for t in wordList for e in t[:-1]) + 1 \n",
    "    format=('%%-%ds' % width) * len(wordList[0])\n",
    "    print '\\n'.join(format % tuple(t) for t in wordList)\n",
    "    print('\\n Total Word Count : %d') %(len(Corpus))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(Corpus,ds,keyword='cognition'):\n",
    "    \"\"\"\n",
    "    Returns the most similar words, determined by neighborhood in corpus\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    Corpus : concatenated list of abstracts\n",
    "    ds : ouptut of lisc\n",
    "    keyword : query\n",
    "    \n",
    "    NOTE: There is a bug in text.similarity in NLTK2.0, we want to use this function to return similar \n",
    "    words in a ranked fashion \n",
    "    \"\"\"\n",
    "    #similar_words = Corpus.similar(keyword)\n",
    "    #return similar_words\n",
    "    #similar_words = Corpus._word_contex_index.similar_words(keyword)\n",
    "    #if similar_words != None:\n",
    "     #   print '\\n'.join(word for word in similar_words)\n",
    "    #else: \n",
    "     #   print 'No matches.'\n",
    "        \n",
    "    idx = nltk.text.ContextIndex([word.lower() for word in Corpus])\n",
    "    simWords = []\n",
    "    for word in nltk.word_tokenize(keyword):\n",
    "        simWords.append(idx.similar_words(word))\n",
    "        if word != None:\n",
    "            print '\\n'.join(idx.similar_words(word))\n",
    "    return list(itertools.chain.from_iterable(simWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just a toy example for now \n",
    "CA_words = ['abstract reasoning','attention','arousal']\n",
    "def CA_word_presence(Corpus,CA_terms):\n",
    "    \"\"\"\n",
    "    Returns the words from CA that are present in the CA\n",
    "    \n",
    "    TO DO: Generalize to entering tasks \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    Corpus : concatenated list of abstracts\n",
    "    \"\"\"    \n",
    "    mask = []\n",
    "    for ca_idx in xrange(len(CA_terms)):\n",
    "        if CA_terms[ca_idx] in Corpus:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "            \n",
    "    PresentTerms = list(compress(CA_words, mask))\n",
    "    print '\\n'.join(word for word in PresentTerms)\n",
    "    \n",
    "    return PresentTerms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TO DO:\n",
    "\n",
    "1. Test functions with lisc instead of erpsc\n",
    "2. Find out the CA terms can be checked within the corpus\n",
    "3. Submit a pull request to add a version of this notebook to NHW repo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Corpus = get_corpus(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_common_words(Corpus,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = get_similarity(Corpus,dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_neighbors(Corpus,keyword)\n",
    "    \n",
    "    plain_list = list(Corpus)\n",
    "    max_instances = 10\n",
    "\n",
    "    indices = [i for i, x in enumerate(plain_list) if x == \"p300\"]\n",
    "\n",
    "    check_first = indices[:max_instances]\n",
    "\n",
    "    for i in range(max_instances):\n",
    "        print('Occurence %d of p300 preceded by %s and followed by %s.') \\\n",
    "        %(check_first[i],plain_list[i-1],plain_list[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textList = []\n",
    "    \n",
    "numArticles = dat.n_articles\n",
    "for AIdx in xrange(numArticles):\n",
    "    if dat.words[AIdx] != None:\n",
    "        textList.append(dat.words[AIdx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(textList)\n",
    "dictionary.save('dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in textList]\n",
    "corpora.MmCorpus.serialize('corpus.mm', doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "#ldamodel.save('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = LdaModel.load('topic.model')\n",
    "print(loading.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_new(doc):\n",
    "    one = doc.split()\n",
    "    two = dictionary.doc2bow(one)\n",
    "    return two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "belong = loading[(pre_new('new article that to be classified by trained model!'))]\n",
    "belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = pd.DataFrame(belong,columns=['id','prob']).sort_values('prob',ascending=False)\n",
    "new['topic'] = new['id'].apply(loading.print_topic)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = pd.DataFrame(belong,columns=['id','prob']).sort_values('prob',ascending=False)\n",
    "new['topic'] = new['id'].apply(loading.print_topic)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = gensim.corpora.Dictionary.load('dictionary.dict')\n",
    "c = gensim.corpora.MmCorpus('corpus.mm')\n",
    "lda = gensim.models.LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(lda, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
