{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add erpsc to path\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_path = '/Users/vassiki/Desktop/nhw17/DataDrivenCognitiveOntology/'\n",
    "#sys.path.append(os.getcwd)\n",
    "sys.path.append(my_path)\n",
    "# ^ Update the above with a link to the folder that has 'erpsc' in it, if not currently in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import erpsc\n",
    "from lisc.base import Base\n",
    "from lisc.core.db import SCDB\n",
    "from lisc.data import Data\n",
    "from lisc.data_all import DataAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add data to path\n",
    "db = SCDB(False)\n",
    "#fake_path = '/Users/vassiki/Desktop/nhw17/ERPSC_NLP/notebooks'\n",
    "db.project_path = os.path.join(os.getcwd(), 'dat')\n",
    "#db.project_path = fake_path\n",
    "db.gen_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load raw data from a particular ERP\n",
    "dat = Data('p300')\n",
    "dat.load(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus(ds):\n",
    "    \"\"\"\n",
    "    To flatten all abstract words into a long list \n",
    "    \n",
    "    TO DO: Generalize to making a corpus of authors, or any metric the author wants\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ds : Data structure returned by ERFSC when a query is submitted\n",
    "    \"\"\"\n",
    "    aPreCorpus = []\n",
    "    \n",
    "    numArticles = ds.n_articles\n",
    "    print('Iterating over %d articles for extracting words') %(ds.n_articles)\n",
    "    for AIdx in xrange(numArticles):\n",
    "        if ds.words[AIdx] != None:\n",
    "            aPreCorpus.append(ds.words[AIdx])\n",
    "        \n",
    "    \n",
    "    PreText = list(itertools.chain.from_iterable(aPreCorpus))\n",
    "    \n",
    "    Corpus = nltk.Text(PreText)\n",
    "    \n",
    "    return Corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_common_words(Corpus,NumWords):\n",
    "    \"\"\"\n",
    "    Return the most common words in the concatenated article text corpus\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Corpus: generated from keyword arguments from cognitive atlas\n",
    "    NumWords: Top N words to return\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(NumWords <= len(Corpus))\n",
    "    \n",
    "    fdist = nltk.FreqDist(Corpus)\n",
    "    common_words = fdist.most_common(NumWords)\n",
    "    header = [('Term', 'Frequency'),('-'*len('Term'),'-'*len('Frequency'))]\n",
    "    wordList = header + common_words\n",
    "    \n",
    "    width = max(len(e) for t in wordList for e in t[:-1]) + 1 \n",
    "    format=('%%-%ds' % width) * len(wordList[0])\n",
    "    print '\\n'.join(format % tuple(t) for t in wordList)\n",
    "    print('\\n Total Word Count : %d') %(len(Corpus))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(Corpus,ds,keyword='cognition'):\n",
    "    \"\"\"\n",
    "    Returns the most similar words, determined by neighborhood in corpus\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    Corpus : concatenated list of abstracts\n",
    "    ds : ouptut of lisc\n",
    "    keyword : query\n",
    "    \n",
    "    NOTE: There is a bug in text.similarity in NLTK2.0, we want to use this function to return similar \n",
    "    words in a ranked fashion \n",
    "    \"\"\"\n",
    "    #similar_words = Corpus.similar(keyword)\n",
    "    #return similar_words\n",
    "    #similar_words = Corpus._word_contex_index.similar_words(keyword)\n",
    "    #if similar_words != None:\n",
    "     #   print '\\n'.join(word for word in similar_words)\n",
    "    #else: \n",
    "     #   print 'No matches.'\n",
    "        \n",
    "    idx = nltk.text.ContextIndex([word.lower() for word in Corpus])\n",
    "    simWords = []\n",
    "    for word in nltk.word_tokenize(keyword):\n",
    "        simWords.append(idx.similar_words(word))\n",
    "        if word != None:\n",
    "            print '\\n'.join(idx.similar_words(word))\n",
    "    return list(itertools.chain.from_iterable(simWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just a toy example for now \n",
    "CA_words = ['abstract reasoning','attention','arousal']\n",
    "def CA_word_presence(Corpus,CA_terms):\n",
    "    \"\"\"\n",
    "    Returns the words from CA that are present in the CA\n",
    "    \n",
    "    TO DO: Generalize to entering tasks \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    Corpus : concatenated list of abstracts\n",
    "    \"\"\"    \n",
    "    mask = []\n",
    "    for ca_idx in xrange(len(CA_terms)):\n",
    "        if CA_terms[ca_idx] in Corpus:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "            \n",
    "    PresentTerms = list(compress(CA_words, mask))\n",
    "    print '\\n'.join(word for word in PresentTerms)\n",
    "    \n",
    "    return PresentTerms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TO DO:\n",
    "\n",
    "1. Test functions with lisc instead of erpsc\n",
    "2. Find out the CA terms can be checked within the corpus\n",
    "3. Submit a pull request to add a version of this notebook to NHW repo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over 4992 articles for extracting words\n"
     ]
    }
   ],
   "source": [
    "Corpus = get_corpus(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term       Frequency  \n",
      "----       ---------  \n",
      "p300       9910       \n",
      "patients   3650       \n",
      "amplitude  3050       \n",
      "stimuli    2503       \n",
      "subjects   2444       \n",
      "latency    2436       \n",
      "study      2431       \n",
      "potentials 2408       \n",
      "auditory   2286       \n",
      "task       2268       \n",
      "\n",
      " Total Word Count : 442877\n"
     ]
    }
   ],
   "source": [
    "get_common_words(Corpus,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cognitive\n",
      "working\n",
      "plus\n",
      "integration\n",
      "structure\n",
      "effects\n",
      "potentials\n",
      "age\n",
      "stimuli\n",
      "abilities\n",
      "followed\n",
      "findings\n",
      "p3b\n",
      "executive\n",
      "brain\n",
      "signs\n",
      "morphology\n",
      "curcumin\n",
      "children\n",
      "prodromal\n"
     ]
    }
   ],
   "source": [
    "sim = get_similarity(Corpus,dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cognitive',\n",
       " u'working',\n",
       " u'plus',\n",
       " u'integration',\n",
       " u'structure',\n",
       " u'effects',\n",
       " u'potentials',\n",
       " u'age',\n",
       " u'stimuli',\n",
       " u'abilities',\n",
       " u'followed',\n",
       " u'findings',\n",
       " u'p3b',\n",
       " u'executive',\n",
       " u'brain',\n",
       " u'signs',\n",
       " u'morphology',\n",
       " u'curcumin',\n",
       " u'children',\n",
       " u'prodromal']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurence 31 of p300 preceded by activity and followed by rotation.\n",
      "Occurence 60 of p300 preceded by mental and followed by body.\n",
      "Occurence 77 of p300 preceded by rotation and followed by parts.\n",
      "Occurence 91 of p300 preceded by body and followed by involves.\n",
      "Occurence 174 of p300 preceded by parts and followed by sequential.\n",
      "Occurence 181 of p300 preceded by involves and followed by cognitive.\n",
      "Occurence 194 of p300 preceded by sequential and followed by processes.\n",
      "Occurence 199 of p300 preceded by cognitive and followed by including.\n",
      "Occurence 303 of p300 preceded by processes and followed by visual.\n",
      "Occurence 320 of p300 preceded by including and followed by processing.\n"
     ]
    }
   ],
   "source": [
    "def test_neighbors(Corpus,keyword)\n",
    "    \n",
    "    plain_list = list(Corpus)\n",
    "    max_instances = 10\n",
    "\n",
    "    indices = [i for i, x in enumerate(plain_list) if x == \"p300\"]\n",
    "\n",
    "    check_first = indices[:max_instances]\n",
    "\n",
    "    for i in range(max_instances):\n",
    "        print('Occurence %d of p300 preceded by %s and followed by %s.') \\\n",
    "        %(check_first[i],plain_list[i-1],plain_list[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
